# main.py - NICEGOLD Assistant (L4 GPU + QA Guard + Full Progress Bars)

import os  # [Patch v12.3.9] Ensure os is imported for path.join
import sys
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

import pandas as pd
import gc
import logging
import json  # [Patch v12.4.0] Added for JSON export
from datetime import datetime
from tqdm import tqdm, trange
from nicegold_v5.wfv import (
    run_walkforward_backtest as raw_run,
    merge_equity_curves,
    plot_equity,
    session_performance,
    streak_summary,
)

# Keep backward-compatible name
run_walkforward_backtest = raw_run

from multiprocessing import cpu_count, get_context
import numpy as np
from nicegold_v5.utils import run_auto_wfv, split_by_session
from nicegold_v5.entry import (
    generate_signals_v12_0 as generate_signals,  # [Patch v12.3.9] ensure import
    sanitize_price_columns,
    validate_indicator_inputs,
    rsi,
)
from nicegold_v5.exit import simulate_partial_tp_safe  # [Patch v12.2.x]
from nicegold_v5.config import (  # [Patch v12.3.9] Import SNIPER_CONFIG_Q3_TUNED
    SNIPER_CONFIG_PROFIT,
    SNIPER_CONFIG_Q3_TUNED,
    RELAX_CONFIG_Q3,
)
from nicegold_v5.optuna_tuner import start_optimization
from nicegold_v5.qa import run_qa_guard, auto_qa_after_backtest
from nicegold_v5.utils import (
    safe_calculate_net_change,
    convert_thai_datetime,
    parse_timestamp_safe,
    get_resource_plan,
)
from nicegold_v5.fix_engine import simulate_and_autofix  # [Patch v12.3.9] Added import
# User-provided custom instructions
# *à¸ªà¸™à¸—à¸™à¸²à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™

# --- Advanced Risk Management (Patch C) ---
KILL_SWITCH_DD = 25  # %
MAX_LOT_CAP = 1.0  # [Patch v6.7] à¸ˆà¸³à¸à¸±à¸”à¸‚à¸™à¸²à¸”à¸¥à¸­à¸•à¸ªà¸¹à¸‡à¸ªà¸¸à¸”à¸•à¹ˆà¸­à¹„à¸¡à¹‰


def kill_switch(equity_curve):
    peak = equity_curve[0]
    for eq in equity_curve:
        dd = (peak - eq) / peak * 100
        if dd >= KILL_SWITCH_DD:
            print("[KILL SWITCH] Drawdown limit reached. Backtest halted.")
            return True
        peak = max(peak, eq)
    return False


def apply_recovery_lot(capital, sl_streak, base_lot=0.01):
    if sl_streak >= 2:
        factor = 1 + 0.5 * (sl_streak - 1)
        return round(base_lot * factor, 2)
    return base_lot


def adaptive_tp_multiplier(session):
    if session == "Asia":
        return 1.5
    elif session == "London":
        return 2.0
    elif session == "NY":
        return 2.5
    return 2.0


def get_sl_tp(price, atr, session, direction):
    multiplier = adaptive_tp_multiplier(session)
    sl = price - atr * 1.2 if direction == "buy" else price + atr * 1.2
    tp = price + atr * multiplier if direction == "buy" else price - atr * multiplier
    return sl, tp


def calc_lot_risk(capital, atr, risk_pct=1.5):
    pip_value = 10
    sl_pips = atr * 10
    risk_amount = capital * (risk_pct / 100)
    lot = risk_amount / (sl_pips * pip_value)
    return max(0.01, round(lot, 2))

# Mock CSV integrity check to keep CLI functional even without testing module
def run_csv_integrity_check():
    return True

TRADE_DIR = "/content/drive/MyDrive/NICEGOLD/logs"
M1_PATH = os.getenv(
    "M1_PATH",
    os.path.join(ROOT_DIR, "nicegold_v5", "XAUUSD_M1.csv"),
)
M15_PATH = os.getenv(
    "M15_PATH",
    os.path.join(ROOT_DIR, "nicegold_v5", "XAUUSD_M15.csv"),
)
DATETIME_FORMAT = "%Y-%m-%d %H:%M:%S"
os.makedirs(TRADE_DIR, exist_ok=True)

# [Patch C.2] Enable full RAM mode
MAX_RAM_MODE = True


def maximize_ram():
    if MAX_RAM_MODE:
        try:
            import psutil
            gc.disable()
            print("ðŸš€ MAX_RAM_MODE: ON â€“ GC disabled")
            print(f"âœ… Total RAM: {psutil.virtual_memory().total / 1024**3:.2f} GB")
        except Exception:
            pass
    else:
        gc.collect()


def run_smart_fast_qa():
    """Run a minimal set of tests for ultra-fast QA."""
    import subprocess
    print("\nðŸ¤– Running Smart Fast QA tests...")
    try:
        subprocess.run([
            "pytest",
            "-q",
            "nicegold_v5/tests/test_core_all.py",
        ], check=True)
        print("âœ… Smart Fast QA Passed")
    except Exception as e:
        print(f"âŒ Smart Fast QA Failed: {e}")

def _run_fold(args):
    df, features, label_col, fold_name = args
    # [Patch v16.0.1] Fallback Fix 'Open' column à¹à¸šà¸šà¹€à¸—à¸ž
    if "Open" not in df.columns:
        if "open" in df.columns:
            df.rename(columns={"open": "Open"}, inplace=True)
            print("[Patch v16.0.1] ðŸ› ï¸ Fallback: à¹ƒà¸Šà¹‰ 'open' â†’ 'Open'")
        elif "close" in df.columns:
            df["Open"] = df["close"]
            print("[Patch v16.0.1] ðŸ› ï¸ Fallback: à¹ƒà¸Šà¹‰ 'close' â†’ 'Open'")
        else:
            raise ValueError("[Patch v16.0.1] âŒ à¹„à¸¡à¹ˆà¸¡à¸µ 'Open'/'close' column à¹ƒà¸«à¹‰ fallback")
    trades = raw_run(df, features, label_col, strategy_name=str(fold_name))
    trades["fold"] = fold_name
    return trades

def run_parallel_wfv(df: pd.DataFrame, features: list, label_col: str, n_folds: int = 5):
    print("\nâš¡ Parallel Walk-Forward (Full RAM Mode)")
    df = df.copy(deep=False)  # [Perf-C] à¸¥à¸” RAM à¹ƒà¸Šà¹‰ deepcopy
    # [Patch v16.0.1] Fallback Fix 'Open' column à¹à¸šà¸šà¹€à¸—à¸ž
    if "Open" not in df.columns:
        if "open" in df.columns:
            df.rename(columns={"open": "Open"}, inplace=True)
            print("[Patch v16.0.1] ðŸ› ï¸ Fallback: à¹ƒà¸Šà¹‰ 'open' â†’ 'Open'")
        elif "close" in df.columns:
            df["Open"] = df["close"]
            print("[Patch v16.0.1] ðŸ› ï¸ Fallback: à¹ƒà¸Šà¹‰ 'close' â†’ 'Open'")
        else:
            raise ValueError("[Patch v16.0.1] âŒ à¹„à¸¡à¹ˆà¸¡à¸µ 'Open'/'close' column à¹ƒà¸«à¹‰ fallback")
    df = df.astype({col: np.float32 for col in features if col in df.columns})
    df[label_col] = df[label_col].astype(np.uint8)
    required_cols = ['open']  # [Patch] Include lowercase 'open' for renaming
    df = df.drop(columns=[col for col in df.columns if col not in features + [label_col] + required_cols])

    session_dict = split_by_session(df)
    args_list = [(sess_df, features, label_col, name) for name, sess_df in session_dict.items()]

    ctx = get_context("spawn")
    try:
        with ctx.Pool(min(cpu_count(), len(args_list))) as pool:
            trades_list = pool.map(_run_fold, args_list)
    except Exception:
        # Fallback to sequential if multiprocessing fails
        trades_list = [_run_fold(args) for args in args_list]

    all_df = pd.concat(trades_list, ignore_index=True)
    out_path = os.path.join(TRADE_DIR, "manual_backtest_trades.csv")
    all_df.to_csv(out_path, index=False)
    print(f"ðŸ“¦ Saved trades to: {out_path}")
    maximize_ram()
    return all_df


def load_csv_safe(path, lowercase=True):
    """Load CSV with fallback to local data directory."""
    if not os.path.exists(path):
        alt = os.path.join(os.path.dirname(__file__), "nicegold_v5", os.path.basename(path))
        if os.path.exists(alt):
            path = alt
    try:
        with tqdm(total=1, desc=f"ðŸ“¥ Loading {os.path.basename(path)}") as pbar:
            df = pd.read_csv(path, engine="python", on_bad_lines="skip")
            if lowercase:
                df.columns = [c.lower().strip() for c in df.columns]
            pbar.update(1)
        print(f"âœ… Loaded {len(df):,} rows from {path}")
        return df
    except Exception as e:
        print(f"âŒ Failed to load {path}: {e}")
        raise



def run_clean_backtest(df: pd.DataFrame) -> pd.DataFrame:
    # [Patch v12.4.2] â€“ Export Summary JSON + QA Summary à¸•à¹ˆà¸­ Fold (Incorporates v12.4.0, v12.4.1)
    # -------------------------------------------------------------------
    # âœ… robust: fallback config if signal missing
    # âœ… export: log + config_used + summary
    # âœ… CLI: à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸¡à¸™à¸¹à¹ƒà¸™ welcome() â†’ choice 7 (commented out in welcome())
    # âœ… à¸šà¸±à¸™à¸—à¸¶à¸ QA Summary à¹à¸¢à¸à¹„à¸Ÿà¸¥à¹Œ JSON
    df = df.copy()
    # à¸£à¸­à¸‡à¸£à¸±à¸š Date+Timestamp à¹à¸šà¸š à¸ž.à¸¨. à¹à¸¥à¸°à¸•à¸±à¸§à¸žà¸´à¸¡à¸žà¹Œà¹€à¸¥à¹‡à¸
    if {"Date", "Timestamp"}.issubset(df.columns):
        df = convert_thai_datetime(df)
        df["timestamp"] = parse_timestamp_safe(df["timestamp"], DATETIME_FORMAT)
    elif {"date", "timestamp"}.issubset(df.columns):
        df["date"] = df["date"].astype(str).str.zfill(8)
        def _th2en(s):
            y, m, d = int(s[:4]) - 543, s[4:6], s[6:8]
            return f"{y:04d}-{m}-{d}"
        df["date_gregorian"] = df["date"].apply(_th2en)
        df["timestamp_full"] = df["date_gregorian"] + " " + df["timestamp"].astype(str)
        df["timestamp"] = pd.to_datetime(df["timestamp_full"], format="%Y-%m-%d %H:%M:%S", errors="coerce")
    df = df.dropna(subset=["timestamp"])
    df = df.sort_values("timestamp")

    # [Patch v12.4.0] Sanitize and validate
    try:
        df = sanitize_price_columns(df)
        validate_indicator_inputs(df, min_rows=min(500, len(df)))
    except TypeError:
        validate_indicator_inputs(df)

    from nicegold_v5.config import RELAX_CONFIG_Q3
    df = generate_signals(df, config=SNIPER_CONFIG_Q3_TUNED)

    # [Patch v12.4.0] Fallback if no signals
    if df["entry_signal"].isnull().mean() >= 1.0:
        print("[Patch QA] âš ï¸ à¹„à¸¡à¹ˆà¸žà¸šà¸ªà¸±à¸à¸à¸²à¸“ â†’ fallback RELAX_CONFIG_Q3")
        df = generate_signals(df, config=RELAX_CONFIG_Q3)

    if "entry_signal" not in df.columns:
        df["entry_signal"] = None

    if df["entry_signal"].isnull().mean() >= 1.0:
        raise RuntimeError("[Patch QA] âŒ à¹„à¸¡à¹ˆà¸¡à¸µà¸ªà¸±à¸à¸à¸²à¸“à¹€à¸‚à¹‰à¸²à¹€à¸¥à¸¢ â€“ à¸«à¸¢à¸¸à¸”à¸£à¸±à¸™ backtest")

    df = df.dropna(subset=["timestamp", "entry_signal", "close", "high", "low"])
    df["entry_time"] = pd.to_datetime(df["timestamp"], errors="coerce")
    df["signal_id"] = df["timestamp"].astype(str)

    print("\nðŸš€ [Patch v12.4.2] Using simulate_and_autofix() pipeline...")
    trades_df, equity_df, config_used = simulate_and_autofix(
        df,
        simulate_partial_tp_safe,
        SNIPER_CONFIG_Q3_TUNED,
        session="London"
    )
    print("\nâœ… Simulation Completed with Adaptive Config:")
    for k, v in config_used.items():
        print(f"    â–¸ {k}: {v}")

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = os.path.join(TRADE_DIR, f"trades_cleanbacktest_{ts}.csv")
    trades_df.to_csv(out_path, index=False)
    print(f"ðŸ“ Exported trades â†’ {out_path}")

    config_path = os.path.join(TRADE_DIR, f"config_used_{ts}.json")
    with open(config_path, "w") as f:
        json.dump(config_used, f, indent=2)
    print(f"âš™ï¸ Exported config_used â†’ {config_path}")

    if "exit_reason" in trades_df.columns:
        qa_summary = {
            "tp1_count": int(trades_df["exit_reason"].eq("tp1").sum()),
            "tp2_count": int(trades_df["exit_reason"].eq("tp2").sum()),
            "sl_count": int(trades_df["exit_reason"].eq("sl").sum()),
            "total_trades": len(trades_df),
            "net_pnl": float(trades_df["pnl"].sum() if "pnl" in trades_df.columns else 0.0),
        }
        qa_path = os.path.join(TRADE_DIR, f"qa_summary_{ts}.json")
        with open(qa_path, "w") as f:
            json.dump(qa_summary, f, indent=2)
        print(f"ðŸ“Š Exported QA Summary â†’ {qa_path}")

        print("\nðŸ“Š [Patch QA] Summary (TP1/TP2):")
        for k, v_val in qa_summary.items():
            print(f"    â–¸ {k.replace('_', ' ').title()}: {v_val}")
    return trades_df

def run_wfv_with_progress(df, features, label_col):
    # [Patch vWFV.7] Fallback support for lowercase 'open' or only 'close'
    if "Open" not in df.columns:
        if "open" in df.columns:
            df = df.rename(columns={"open": "Open"})
            print("[Patch vWFV.7] ðŸ› ï¸ rename 'open' â†’ 'Open'")
        elif "close" in df.columns:
            df = df.copy()
            df["Open"] = df["close"]
            print("[Patch vWFV.7] ðŸ› ï¸ create 'Open' from 'close'")
        else:
            raise ValueError("[Patch vWFV.7] âŒ Missing 'Open'/'open'/'close'")
    from nicegold_v5.utils import split_by_session

    logging.info("[TIME] run_wfv_with_progress(): Start")

    session_folds = split_by_session(df)
    all_trades = []
    print("\nðŸ“Š Running Session Folds:")
    for name, sess_df in session_folds.items():
        fold_pbar = tqdm(total=1, desc=f"ðŸ” {name}", unit="step")
        try:
            trades = run_walkforward_backtest(sess_df, features, label_col, strategy_name=name)
            if not trades.empty:
                trades["fold"] = name
                start_time = trades["time"].min() if "time" in trades.columns else "N/A"
                end_time = trades["time"].max() if "time" in trades.columns else "N/A"
                duration_days = (pd.to_datetime(end_time) - pd.to_datetime(start_time)).days if start_time != "N/A" else "-"
                num_orders = len(trades)
                total_lots = trades["lot"].sum() if "lot" in trades.columns else 0
                total_pnl = trades["pnl"].sum()
                win_trades = trades[trades["pnl"] > 0].shape[0]
                loss_trades = trades[trades["pnl"] < 0].shape[0]
                max_dd = trades["drawdown"].max() if "drawdown" in trades.columns else None

                print(f"ðŸ“ˆ {name} Summary:")
                print(f"    â–¸ Orders     : {num_orders}")
                print(f"    â–¸ Total Lots : {total_lots:.2f}")
                print(f"    â–¸ Win Trades : {win_trades} | Loss Trades : {loss_trades}")
                print(f"    â–¸ Total PnL  : {total_pnl:.2f} USD")
                print(f"    â–¸ Duration   : {duration_days} days")
                print(f"    â–¸ Max Drawdown: {max_dd:.2%}" if max_dd is not None else "")
            all_trades.append(trades)
            fold_pbar.update(1)
            maximize_ram()
        except Exception as e:
            print(f"âŒ Error in {name}: {e}")
    logging.info("[TIME] run_wfv_with_progress(): Done")
    return pd.concat(all_trades, ignore_index=True) if all_trades else pd.DataFrame()

def show_progress_bar(task_desc, steps=5):
    for _ in trange(steps, desc=task_desc, unit="step"):
        pass


def autopipeline(mode="default", train_epochs=1):
    """Run full ML + AutoFix WFV pipeline automatically."""
    print("\nðŸš€ à¹€à¸£à¸´à¹ˆà¸¡ NICEGOLD AutoPipeline")
    maximize_ram()

    try:
        from nicegold_v5.ml_dataset_m1 import generate_ml_dataset_m1
        from nicegold_v5.train_lstm_runner import load_dataset, train_lstm
        import torch
    except Exception:
        torch = None
        generate_ml_dataset_m1 = None
        load_dataset = None
        train_lstm = None
        print("âš ï¸ PyTorch à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ - à¸‚à¹‰à¸²à¸¡à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™ LSTM")

    plan = get_resource_plan()
    device = plan["device"]
    DEVICE = torch.device(device) if torch else None
    print("\nðŸ§  AI Resource Plan Summary:")
    print(f"   â–¸ GPU       : {plan['gpu']}")
    print(f"   â–¸ RAM       : {plan['ram']:.2f} GB")
    print(f"   â–¸ VRAM      : {plan['vram']:.2f} GB")
    print(f"   â–¸ CUDA Core : {plan['cuda_cores']}")
    print(f"   â–¸ Threads   : {plan['threads']}")
    print(f"   â–¸ Precision : {plan['precision']}")
    print(f"   â–¸ Batch     : {plan['batch_size']}")
    print(f"   â–¸ ModelDim  : {plan['model_dim']}")
    print(f"   â–¸ Epochs    : {plan['train_epochs']}")
    print(f"   â–¸ Optimizer : {plan['optimizer']}")
    print("âœ… à¸šà¸±à¸™à¸—à¸¶à¸ resource_plan.json à¹à¸¥à¹‰à¸§à¸—à¸µà¹ˆ logs/")

    batch_size = plan["batch_size"]
    model_dim = plan["model_dim"]
    n_folds = plan["n_folds"]
    lr = plan.get("lr", 0.001)
    opt = plan["optimizer"]
    train_epochs = plan.get("train_epochs", train_epochs)

    # Load and prepare CSV for all pipeline modes
    df = load_csv_safe(M1_PATH)
    df = convert_thai_datetime(df)
    df["timestamp"] = parse_timestamp_safe(df["timestamp"], DATETIME_FORMAT)
    df = sanitize_price_columns(df)
    try:
        validate_indicator_inputs(df, min_rows=min(500, len(df)))
    except TypeError:
        validate_indicator_inputs(df)
    df = generate_signals(df, config=SNIPER_CONFIG_Q3_TUNED)
    if df["entry_signal"].isnull().mean() >= 1.0:
        print("[AutoPipeline] âš ï¸ à¹„à¸¡à¹ˆà¸¡à¸µà¸ªà¸±à¸à¸à¸²à¸“ â€“ fallback RELAX_CONFIG_Q3")
        df = generate_signals(df, config=RELAX_CONFIG_Q3)

    if mode == "ai_master" and torch is not None:
        print("\nðŸ§  [AI Master Pipeline] à¹€à¸£à¸´à¹ˆà¸¡à¸—à¸³à¸‡à¸²à¸™à¹à¸šà¸šà¸„à¸£à¸šà¸§à¸‡à¸ˆà¸£ (SHAP + Optuna + Guard + WFV)")
        generate_ml_dataset_m1(csv_path=M1_PATH, out_path="data/ml_dataset_m1.csv")
        X, y = load_dataset("data/ml_dataset_m1.csv")
        print(f"[Debug] y unique: {y.unique()}, value_counts: {np.unique(y, return_counts=True)}")
        model = train_lstm(
            X,
            y,
            hidden_dim=model_dim,
            epochs=train_epochs,
            lr=lr,
            batch_size=batch_size,
            optimizer_name=opt,
        )
        os.makedirs("models", exist_ok=True)
        torch.save(model.state_dict(), "models/model_lstm_tp2.pth")
        print("âœ… à¸šà¸±à¸™à¸—à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥ LSTM à¹à¸¥à¹‰à¸§")

        try:
            import shap

            explainer = shap.DeepExplainer(model, X[:100])
            shap_vals = explainer.shap_values(X[:100])[0]
            shap_mean = np.abs(shap_vals).mean(axis=0)
            top_k_idx = np.argsort(shap_mean)[::-1][:5]
            all_features = [
                "gain_z",
                "ema_slope",
                "atr",
                "rsi",
                "volume",
                "entry_score",
                "pattern_label",
            ]
            top_features = [all_features[i] for i in top_k_idx]
            print("ðŸ“Š [Patch v24.1.0] SHAP Top Features:", top_features)
            shap.summary_plot(shap_vals, X[:100], feature_names=top_features, show=False)
            import matplotlib.pyplot as plt
            plt.savefig("logs/shap_summary.png")
            with open("logs/shap_top_features.json", "w") as f:
                json.dump(top_features, f, indent=2)
        except Exception as e:
            print(f"âš ï¸ [Patch v24.1.0] SHAP skipped: {e}")
            top_features = [
                "gain_z",
                "ema_slope",
                "atr",
                "rsi",
                "volume",
                "entry_score",
                "pattern_label",
            ]

        # [Patch v24.1.0] âœ… à¹€à¸—à¸£à¸™ MetaClassifier à¸ˆà¸²à¸ trade log
        try:
            from sklearn.ensemble import RandomForestClassifier
            import joblib
            trades_meta = pd.read_csv("logs/trades_v12_tp1tp2.csv")
            trades_meta = trades_meta.dropna(subset=["exit_reason", "entry_score"])
            trades_meta["target"] = trades_meta["exit_reason"].eq("tp2").astype(int)
            features = ["gain_z", "ema_slope", "atr", "rsi", "entry_score"]
            clf = RandomForestClassifier(n_estimators=100, random_state=42)
            clf.fit(trades_meta[features], trades_meta["target"])
            os.makedirs("models", exist_ok=True)
            joblib.dump(clf, "models/meta_exit.pkl")
            print("âœ… [MetaClassifier] à¸šà¸±à¸™à¸—à¸¶à¸ model â†’ models/meta_exit.pkl")
        except Exception as e:
            print(f"âš ï¸ [MetaClassifier] Training Failed: {e}")

        # [Patch v24.1.0] âœ… à¹€à¸—à¸£à¸™ RL Agent à¸ˆà¸²à¸ trade log
        try:
            from nicegold_v5.rl_agent import RLScalper
            trades_rl = trades_meta.copy()
            agent = RLScalper()
            for i in range(len(trades_rl) - 1):
                s = int(trades_rl.iloc[i]["gain_z"] > 0)
                ns = int(trades_rl.iloc[i + 1]["gain_z"] > 0)
                reward = (
                    1
                    if trades_rl.iloc[i]["exit_reason"] == "tp2"
                    else -1
                    if "sl" in trades_rl.iloc[i]["exit_reason"]
                    else 0.5
                )
                action = 0 if trades_rl.iloc[i]["entry_signal"] == "buy" else 1
                agent.update(s, action, reward, ns)
            import pickle

            with open("models/rl_agent.pkl", "wb") as f:
                pickle.dump(agent, f)
            print("âœ… [RL Agent] à¸šà¸±à¸™à¸—à¸¶à¸ model â†’ models/rl_agent.pkl")
        except Exception as e:
            print(f"âš ï¸ [RL Trainer] Failed: {e}")

        df_feat = pd.read_csv("data/ml_dataset_m1.csv")
        df_feat = df_feat.dropna(subset=top_features)
        df_feat = df_feat[top_features + ["timestamp", "tp2_hit"]]
        study = start_optimization(df, n_trials=100)
        print("âœ… [Patch v22.7.2] Optuna à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ â€“ à¸šà¸±à¸™à¸—à¸¶à¸ best trial")
        with open("logs/optuna_best_config.json", "w") as f:
            json.dump(study.best_trial.params, f, indent=2)

        df = load_csv_safe(M1_PATH)
        df = convert_thai_datetime(df)
        df["timestamp"] = parse_timestamp_safe(df["timestamp"], DATETIME_FORMAT)
        df = sanitize_price_columns(df)
        df = generate_signals(df, config=study.best_trial.params)

        seq_len = 10
        data = df_feat[top_features].values
        seqs = [data[i : i + seq_len] for i in range(len(data) - seq_len)]
        device2 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.eval()
        with torch.no_grad():
            X_tensor = torch.tensor(np.array(seqs), dtype=torch.float32).to(device2)
            preds = model(X_tensor).squeeze().cpu().numpy()
        df_feat["tp2_proba"] = np.concatenate([np.zeros(seq_len), preds])

        df = df.merge(df_feat[["timestamp", "tp2_proba"]], on="timestamp", how="left")
        df["tp2_guard_pass"] = df["tp2_proba"] >= 0.7
        print(
            f"âœ… [Patch v22.7.2] TP2 Guard Filter â†’ à¹€à¸«à¸¥à¸·à¸­ {df['entry_signal'].notnull().sum()} signals"
        )
        df = df[df["tp2_guard_pass"] | df["entry_signal"].isnull()]

        from nicegold_v5.utils import run_autofix_wfv

        trades_df = run_autofix_wfv(df, simulate_partial_tp_safe, SNIPER_CONFIG_Q3_TUNED)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        out_path = os.path.join(TRADE_DIR, f"trades_ai_master_{ts}.csv")
        trades_df.to_csv(out_path, index=False)
        print(f"ðŸ“¦ [Patch v22.7.2] Exported trades â†’ {out_path}")
        return trades_df

    # [Patch v24.0.0] âœ… à¹€à¸žà¸´à¹ˆà¸¡ AI Fusion Mode: LSTM + SHAP + Meta + RL Fallback
    if mode == "fusion_ai" and torch is not None:
        print("\nðŸš€ [Fusion AI] à¹€à¸£à¸´à¹ˆà¸¡ Pipeline à¹à¸šà¸š AI Fusion Strategy Engine")
        from nicegold_v5.ml_dataset_m1 import generate_ml_dataset_m1
        from nicegold_v5.train_lstm_runner import load_dataset, train_lstm
        import shap

        generate_ml_dataset_m1(csv_path=M1_PATH, out_path="data/ml_dataset_m1.csv")
        X, y = load_dataset("data/ml_dataset_m1.csv")
        print(f"[Debug] y unique: {y.unique()}, value_counts: {np.unique(y, return_counts=True)}")
        model = train_lstm(X, y, epochs=train_epochs)
        os.makedirs("models", exist_ok=True)
        torch.save(model.state_dict(), "models/model_lstm_tp2.pth")
        print("âœ… à¸šà¸±à¸™à¸—à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥ TP2 LSTM à¹à¸¥à¹‰à¸§")

        explainer = shap.DeepExplainer(model, X[:100])
        shap_vals = explainer.shap_values(X[:100])[0]
        shap_mean = np.abs(shap_vals).mean(axis=0)
        all_features = ["gain_z", "ema_slope", "atr", "rsi", "volume", "entry_score", "pattern_label"]
        top_k_idx = np.argsort(shap_mean)[::-1][:5]
        top_features = [all_features[i] for i in top_k_idx]
        print("ðŸ“Š [SHAP] Top Features:", top_features)
        with open("logs/shap_top_features.json", "w") as f:
            json.dump(top_features, f)

        df_feat = pd.read_csv("data/ml_dataset_m1.csv")
        df_feat["tp2_proba"] = np.concatenate([np.zeros(10), model(X).detach().cpu().numpy().flatten()])
        df_feat = df_feat[["timestamp", "tp2_proba"] + top_features]

        df_raw = load_csv_safe(M1_PATH)
        df_raw = sanitize_price_columns(df_raw)
        df_raw = generate_signals(df_raw, config=SNIPER_CONFIG_Q3_TUNED)
        df = df_raw.merge(df_feat, on="timestamp", how="left")
        df["tp2_guard_pass"] = df["tp2_proba"] >= 0.7
        df = df[df["tp2_guard_pass"] | df["entry_signal"].isnull()]
        print(f"âœ… TP2 Guard Filter â†’ à¹€à¸«à¸¥à¸·à¸­ {df['entry_signal'].notnull().sum()} signals")

        try:
            from nicegold_v5.meta_classifier import MetaClassifier
            meta = MetaClassifier("model/meta_exit.pkl")
            df_meta = df[df["entry_signal"].notnull()].copy()
            df_meta["meta_decision"] = meta.predict(df_meta)
            df = df_meta[df_meta["meta_decision"] == 1]
            print(f"âœ… MetaClassifier à¹€à¸«à¸¥à¸·à¸­ {len(df)} signals")
        except Exception:
            print("âš ï¸ MetaClassifier à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡ â†’ à¸‚à¹‰à¸²à¸¡")

        if df.empty:
            from nicegold_v5.rl_agent import RLScalper
            agent = RLScalper()
            df_rl = load_csv_safe(M1_PATH)
            agent.train(df_rl)
            print("âœ… RL Fallback à¸¢à¸´à¸‡à¹„à¸¡à¹‰à¹à¸šà¸š Q-Learning")
            df_rl["entry_signal"] = df_rl.apply(lambda r: "buy" if agent.act(int(r["close"] > r["open"])) == 0 else "sell", axis=1)
            df = df_rl

        from nicegold_v5.utils import run_autofix_wfv
        trades_df = run_autofix_wfv(df, simulate_partial_tp_safe, SNIPER_CONFIG_Q3_TUNED, n_folds=5)
        out_path = os.path.join(TRADE_DIR, "trades_fusion_ai_" + datetime.now().strftime("%Y%m%d_%H%M%S") + ".csv")
        trades_df.to_csv(out_path, index=False)
        print(f"ðŸ“¦ Exported FusionAI trades â†’ {out_path}")
        maximize_ram()
        return trades_df

    if torch is not None and mode in ["full", "ultra"]:
        print(
            f"âš™ï¸ [mode={mode}] Training LSTM {train_epochs} epochs hidden_dim={model_dim} batch_size={batch_size} lr={lr} optimizer={opt}"
        )
        # Step 2: Generate ML Dataset safely (after timestamp is confirmed)
        try:
            generate_ml_dataset_m1(csv_path=M1_PATH, out_path="data/ml_dataset_m1.csv")
        except Exception as e:
            print("âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸£à¹‰à¸²à¸‡ dataset à¹„à¸”à¹‰:", e)
            return

        # Step 3: Train LSTM Classifier
        X, y = load_dataset("data/ml_dataset_m1.csv")
        print(f"[Debug] y unique: {y.unique()}, value_counts: {np.unique(y, return_counts=True)}")
        model = train_lstm(
            X,
            y,
            hidden_dim=model_dim,
            epochs=train_epochs,
            lr=lr,
            batch_size=batch_size,
            optimizer_name=opt,
        )
        os.makedirs("models", exist_ok=True)
        torch.save(model.state_dict(), "models/model_lstm_tp2.pth")
        print("âœ… à¸šà¸±à¸™à¸—à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆ models/model_lstm_tp2.pth")

        df_feat = pd.read_csv("data/ml_dataset_m1.csv")
        df_feat["timestamp"] = parse_timestamp_safe(df_feat["timestamp"], DATETIME_FORMAT)
        df_feat["timestamp"] = pd.to_datetime(df_feat["timestamp"], errors="coerce")
        df_feat = df_feat.dropna(subset=["timestamp"])
        feat_cols = [
            "gain_z",
            "ema_slope",
            "atr",
            "rsi",
            "volume",
            "entry_score",
            "pattern_label",
        ]
        data = df_feat[feat_cols].values
        seq_len = 10
        seqs = [data[i : i + seq_len] for i in range(len(data) - seq_len)]
        if seqs:
            X_tensor = torch.tensor(np.array(seqs), dtype=torch.float32).to(DEVICE)
            model = model.to(DEVICE)
            model.eval()
            with torch.no_grad():
                pred = model(X_tensor).squeeze().cpu().numpy()
            df_feat["tp2_proba"] = np.concatenate([np.zeros(seq_len), pred])
        else:
            df_feat["tp2_proba"] = 0.0

        df = df.merge(df_feat[["timestamp", "tp2_proba"]], on="timestamp", how="left")
        df["tp2_guard_pass"] = df["tp2_proba"] >= 0.7
        df = df[df["tp2_guard_pass"] | df["entry_signal"].isnull()]
        print(f"âœ… TP2 Guard Filter â†’ à¹€à¸«à¸¥à¸·à¸­ {df['entry_signal'].notnull().sum()} signals")
    else:
        n_folds = plan["n_folds"]

    from nicegold_v5.utils import run_autofix_wfv
    trades_df = run_autofix_wfv(
        df,
        simulate_partial_tp_safe,
        SNIPER_CONFIG_Q3_TUNED,
        n_folds=n_folds,
    )
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = os.path.join(TRADE_DIR, f"trades_autopipeline_{ts}.csv")
    trades_df.to_csv(out_path, index=False)
    print(f"ðŸ“¦ Exported AutoPipeline trades â†’ {out_path}")
    maximize_ram()
    return trades_df

def welcome():
    print("\nðŸŸ¡ NICEGOLD Assistant à¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸«à¹‰à¸šà¸£à¸´à¸à¸²à¸£à¹à¸¥à¹‰à¸§ (L4 GPU + QA Guard)")
    maximize_ram()

    print("\nðŸ§© à¹€à¸¥à¸·à¸­à¸à¹€à¸¡à¸™à¸¹:")
    print("1. ðŸš€ Full AutoPipeline Mode (ML+Optuna+LSTM+SHAP+RL+WFV)")
    print("2. ðŸŸ¢ Smart Fast QA (Ultra-Fast Logic Test)")
    choice = input("ðŸ‘‰ à¹€à¸¥à¸·à¸­à¸à¹€à¸¡à¸™à¸¹ (1â€“2): ").strip()
    try:
        choice = int(choice)
    except Exception:
        print("âŒ à¹€à¸¡à¸™à¸¹à¹„à¸¡à¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡")
        return

    if choice == 1:
        print("\nðŸš€ [Full AutoPipeline] à¹‚à¸«à¸¡à¸” ML+Optuna+LSTM+SHAP+RL+WFV à¸„à¸£à¸šà¸—à¸¸à¸à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œ")
        autopipeline(mode="ai_master", train_epochs=50)
        maximize_ram()
        return
    elif choice == 2:
        print("\nðŸŸ¢ [Smart Fast QA Mode] à¸£à¸±à¸™à¸—à¸”à¸ªà¸­à¸š ultra-fast logic...")
        run_smart_fast_qa()
        maximize_ram()
        return
    else:
        print("âŒ à¹€à¸¥à¸·à¸­à¸à¹€à¸¡à¸™à¸¹à¹„à¸¡à¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡")
        maximize_ram()
        return

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "clean":
        print("ðŸ“¥ Loading CSV...")
        df = load_csv_safe(M1_PATH)
        df = convert_thai_datetime(df)
        df["timestamp"] = parse_timestamp_safe(df["timestamp"], DATETIME_FORMAT)
        df = df.dropna(subset=["timestamp"])
        run_clean_backtest(df)
        print("âœ… Done: Clean Backtest Completed")
    else:
        welcome()
    # [Patch v12.4.1] Example of how choice 7 might be handled if menu was active
    # elif choice == 7:  # This would be part of the active menu loop in welcome()
    #     print("\nðŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸£à¸±à¸™ CleanBacktest à¸”à¹‰à¸§à¸¢ AutoFix + Export...")
    #     df = load_csv_safe(M1_PATH)  # Ensure M1_PATH is defined
    #     # Potentially convert_thai_datetime(df) and other pre-processing here
    #     trades_df = run_clean_backtest(df)
